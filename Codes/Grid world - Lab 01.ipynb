{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPFbTA/xAguhGNlsZSiLm0Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"KjAwMCTfSUjs","executionInfo":{"status":"ok","timestamp":1705580162406,"user_tz":-330,"elapsed":1720,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}}},"outputs":[],"source":["import itertools\n","import numpy as np\n","import pandas as pd\n","import random"]},{"cell_type":"markdown","source":["  ## Grid world\n","  It has N x M states. The Agent can move from one state to another state depending on the state it is currently in and the action it takes.\n","\n","  Here, there are total of 5*5 = 25 states representing the \"environment\".\n","  \n","  |   |  0  |  1  |  2  |  3  |  4  |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |(0,0)|(0,1)|(0,2)|(0,3)|(0,4)|\n","  | 1 |(1,0)|(1,1)|(1,2)|(1,3)|(1,4)|\n","  | 2 |(2,0)|(2,1)|(2,2)|(2,3)|(2,4)|\n","  | 3 |(3,0)|(3,1)|(3,2)|(3,3)|(3,4)|\n","  | 4 |(4,0)|(4,1)|(4,2)|(4,3)|(4,4)|\n","  \n","____________________________\n","### Example\n","**agent is at (2,2)**:\n","\n","\n","  |   |  0  |  1  |  2  |  3  |  4  |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |     |     |     |     |     |\n","  | 1 |     |     |     |     |     |\n","  | 2 |     |     |[ A ]|     |     |\n","  | 3 |     |     |     |     |     |\n","  | 4 |     |     |     |     | END |\n","\n","\n","Takes an action to move ***right***. Action: (0,1)\n","\n","next_state = current_state + action = (2,2) + (0,1) = (2,3)\n","\n","  |   |  0  |  1  |  2  |  3  |  4  |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |     |     |     |     |     |\n","  | 1 |     |     |     |     |     |\n","  | 2 |     |     |     |[ A ]|     |\n","  | 3 |     |     |     |     |     |\n","  | 4 |     |     |     |     | END |\n","\n","\n","Takes an action to move ***down***. Action: (1,0)\n","\n","next_state = current_state + action = (2,3) + (1,3) = (3,3)\n","\n","  |   |  0  |  1  |  2  |  3  |  4  |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |     |     |     |     |     |\n","  | 1 |     |     |     |     |     |\n","  | 2 |     |     |     |     |     |\n","  | 3 |     |     |     |[ A ]|     |\n","  | 4 |     |     |     |     | END |\n","\n","\n","Takes an action to move ***right***. Action: (0,1)\n","\n","next_state = current_state + action = (3,3) + (0,1) = (3,4)\n","\n","  |   |  0  |  1  |  2  |  3  |  4  |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |     |     |     |     |     |\n","  | 1 |     |     |     |     |     |\n","  | 2 |     |     |     |     |     |\n","  | 3 |     |     |     |     |[ A ]|\n","  | 4 |     |     |     |     | END |\n","\n","\n","Takes an action to move ***down***. Action: (1,0)\n","\n","next_state = current_state + action = (3,4) + (1,0) = (4,4)\n","\n","  |   |  0  |  1  |  2  |  3  |  4  |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |     |     |     |     |     |\n","  | 1 |     |     |     |     |     |\n","  | 2 |     |     |     |     |     |\n","  | 3 |     |     |     |     |     |\n","  | 4 |     |     |     |     |[ A ]|"],"metadata":{"id":"mutfQDpOWDph"}},{"cell_type":"code","source":["class GridWorldEnv:\n","  \"\"\"Grid world is an one of most classical reinforcement learning problems.\n","  Here the envirnment is made of a rectangular 2-d box with N rows and M columns,\n","  totally consisting of N x M states.\n","\n","  The agent at any given point can take one of the 4 possible actions - to move:\n","  - Left (0,-1)\n","  - Right (0,1)\n","  - Up (-1,0)\n","  - Down (1,0)\n","\n","  Different types of rewards and constraints can be formulated in this kind of\n","  setup\n","\n","  Reward\n","  ------\n","  - Moving to the terminal state receives a reward of +10\n","  - Every other step receives -1 reward.\n","\n","  Teminal states:\n","  (4,4)\n","\n","  |   | 0   | 1   | 2   | 3   | 4   |\n","  |---|-----|-----|-----|-----|-----|\n","  | 0 |start|     |     |     |     |\n","  | 1 |     |     |     |     |     |\n","  | 2 |     |     |     |     |     |\n","  | 3 |     |     |     |     |     |\n","  | 4 |     |     |     |     | END |\n","\n","  \"\"\"\n","  def __init__(self, N = 10, M = 10):\n","    self.N = N\n","    self.M = M\n","    self.total_states = N*M\n","\n","    self.observation_space = list(itertools.product(range(N), range(M)))\n","    self.action_space = [(0,1), (1,0), (0, -1), (-1, 0)]\n","\n","    self.terminated = False\n","    self.total_reward = 0\n","\n","    # once the agent reaches terminal state, it stays there. Assume start state cannot be terminal state.\n","    # also known as absorbing states\n","    self.terminal_states = [(4,4)]\n","    self.reset()\n","\n","  def reset(self):\n","    self.state = (0,0)\n","\n","  def _get_transition_probability(self, present_state, action, next_state):\n","\n","    if present_state in self.terminal_states:\n","      return 0\n","\n","    # If the expected state matches the action taken, return a probability 1\n","    expected_state = tuple(np.array(present_state) + np.array(action))\n","    if expected_state == next_state:\n","      return 1\n","\n","    # Make sure the agent does not go out of the grid\n","    if (expected_state not in self.observation_space\n","        and present_state == next_state):\n","      return 1\n","\n","    return 0\n","\n","  def _get_reward(self,present_state, action, next_state):\n","    if next_state in self.terminal_states:\n","      reward = 10\n","    else:\n","      reward = -1\n","    return reward\n","\n","  def step(self, action):\n","    # check if terminal state is reached\n","    if self.state in self.terminal_states:\n","      self.terminated = True\n","      reward = np.nan\n","      return self.state, reward, self.terminated\n","\n","    max_prob = -np.inf\n","    for possible_state in self.observation_space:\n","      p = self._get_transition_probability(self.state, action, possible_state)\n","      if p > max_prob:\n","        next_state = possible_state\n","        max_prob = p\n","\n","    reward = self._get_reward(self.state, action, next_state)\n","\n","    self.state = next_state\n","    self.total_reward = self.total_reward + reward\n","\n","    return self.state, reward, self.terminated"],"metadata":{"id":"99Xjz2tSTbyL","executionInfo":{"status":"ok","timestamp":1705583258715,"user_tz":-330,"elapsed":4,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["class Agent:\n","  def __init__(self,env):\n","    self.env = env\n","    env.reset()\n","\n","  def policy(self):\n","    current_state = self.env.state\n","    action = random.choice([(0,1), (1,0)])\n","    # action = (0, 1)\n","    return action"],"metadata":{"id":"IMBgnuazTpYJ","executionInfo":{"status":"ok","timestamp":1705584639426,"user_tz":-330,"elapsed":460,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["env = GridWorldEnv(5,5)\n","agent = Agent(env)\n","\n","agent.policy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZLSw_TWUK5n","executionInfo":{"status":"ok","timestamp":1705584642552,"user_tz":-330,"elapsed":386,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"642a4eb5-41f6-436b-a64a-c9856f883ddb"},"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 1)"]},"metadata":{},"execution_count":78}]},{"cell_type":"code","source":["agent.policy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VkRxBU_1U0fy","executionInfo":{"status":"ok","timestamp":1705583813512,"user_tz":-330,"elapsed":5,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"6cc873fe-e7a2-46be-c03b-c091b2d0b1c2"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 1)"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["env.state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"glohNV6arLbg","executionInfo":{"status":"ok","timestamp":1705583310609,"user_tz":-330,"elapsed":409,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"03e473b0-aea9-42b3-8af2-d4ab61ed0035"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0, 0)"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["a = agent.policy()\n","a, env.step(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iAfeVL7QqdOW","executionInfo":{"status":"ok","timestamp":1705583317129,"user_tz":-330,"elapsed":405,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"162f64ae-b81f-4765-c61a-b4755dad5732"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((0, 1), ((0, 1), -1, False))"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["# random walk\n","# get total reward\n","# try with different policies\n","episodes = 100\n","\n","reward_sum = 0\n","for i in range(episodes):\n","  env = GridWorldEnv(5,5)\n","  agent = Agent(env)\n","  while not env.terminated:\n","    current_state = env.state\n","    action = agent.policy()\n","    next_state, reward, terminated = env.step(action)\n","  reward_sum += env.total_reward\n","\n","  # print(\"current_state:\",current_state, \" action: \", action, \"next_state: \", next_state, \"reward: \", reward)\n","\n","print(\"Average total reward: \", reward_sum/episodes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z_bi0j6FUXUI","executionInfo":{"status":"ok","timestamp":1705584753512,"user_tz":-330,"elapsed":702,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"743e9acf-d42f-4386-f3bb-55134e241af4"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["Average total reward:  0.52\n"]}]},{"cell_type":"code","source":["env.total_reward"],"metadata":{"id":"JpfmG2w_YVVI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1705583967288,"user_tz":-330,"elapsed":393,"user":{"displayName":"S Yeshwanth","userId":"01697830625041192506"}},"outputId":"9e75c8f5-c7a0-4082-a974-2e1be40367e0"},"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-100"]},"metadata":{},"execution_count":69}]},{"cell_type":"markdown","source":["### ðŸ“” Tasks\n","- Obtain the average total reward received by taking random actions.\n","- Add dummy states as [(3,0), (2,3)]. Dummy states are states where the agents cannot move to. Hint: modify the transition probability such that the agent cannot move to these states, similar to how we control the agent from moving out of the grid.\n","- Does this change the average total reward received by the agent.\n","\n"],"metadata":{"id":"kBteaFofd_-1"}}]}